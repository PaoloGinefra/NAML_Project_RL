{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for REINFORCE with Baseline implementation\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Environment and utilities\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork:\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], action_dim: int, discrete_bins: int = 5):\n",
    "        \"\"\"\n",
    "        Multi-layer policy network for continuous action spaces via discretization.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: State space dimension\n",
    "            hidden_dims: Hidden layer dimensions\n",
    "            action_dim: Number of continuous action dimensions (8 for Ant)\n",
    "            discrete_bins: Number of discrete bins per action dimension\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.action_dim = action_dim\n",
    "        self.discrete_bins = discrete_bins\n",
    "        \n",
    "        # Total output size: action_dim * discrete_bins\n",
    "        self.output_dim = action_dim * discrete_bins\n",
    "        self.layer_dims = [input_dim] + hidden_dims + [self.output_dim]\n",
    "        \n",
    "        self.params = self._initialize_params()\n",
    "    \n",
    "    def _initialize_params(self) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Initialize weights and biases using Xavier initialization.\"\"\"\n",
    "        key = jax.random.PRNGKey(42)\n",
    "        weights, biases = [], []\n",
    "        \n",
    "        for i in range(len(self.layer_dims) - 1):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            fan_in, fan_out = self.layer_dims[i], self.layer_dims[i + 1]\n",
    "            \n",
    "            # Initialize weights and biases to 0\n",
    "            w = jnp.zeros((fan_in, fan_out))\n",
    "            b = jnp.zeros(fan_out)\n",
    "            \n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "        \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(state: jnp.ndarray, params: Tuple[List[jnp.ndarray], List[jnp.ndarray]], \n",
    "                action_dim: int, discrete_bins: int) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass returning action probabilities for each dimension.\"\"\"\n",
    "        weights, biases = params\n",
    "        x = state\n",
    "        \n",
    "        # Hidden layers with ReLU activation\n",
    "        for w, b in zip(weights[:-1], biases[:-1]):\n",
    "            x = jax.nn.relu(x @ w + b)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = x @ weights[-1] + biases[-1]\n",
    "        \n",
    "        # Reshape to (action_dim, discrete_bins) and apply softmax per action dimension\n",
    "        logits = logits.reshape((action_dim, discrete_bins))\n",
    "        action_probs = jax.nn.softmax(logits, axis=1)\n",
    "        \n",
    "        return action_probs\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_prob(state: jnp.ndarray, actions: jnp.ndarray, params: Tuple[List[jnp.ndarray], List[jnp.ndarray]],\n",
    "                action_dim: int, discrete_bins: int) -> jnp.ndarray:\n",
    "        \"\"\"Compute log probability of actions.\"\"\"\n",
    "        action_probs = PolicyNetwork.forward(state, params, action_dim, discrete_bins)\n",
    "        \n",
    "        # Get log probabilities for selected actions\n",
    "        log_probs = jnp.log(action_probs + 1e-8)\n",
    "        selected_log_probs = log_probs[jnp.arange(action_dim), actions]\n",
    "        \n",
    "        return jnp.sum(selected_log_probs)  # Sum log probs (product of probs)\n",
    "\n",
    "\n",
    "class ValueNetwork:\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"\n",
    "        State-value function network for baseline estimation.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: State space dimension\n",
    "            hidden_dims: Hidden layer dimensions\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.layer_dims = [input_dim] + hidden_dims + [1]  # Output single value\n",
    "        \n",
    "        self.params = self._initialize_params()\n",
    "    \n",
    "    def _initialize_params(self) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Initialize weights and biases using Xavier initialization.\"\"\"\n",
    "        key = jax.random.PRNGKey(123)  # Different seed from policy network\n",
    "        weights, biases = [], []\n",
    "        \n",
    "        for i in range(len(self.layer_dims) - 1):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            fan_in, fan_out = self.layer_dims[i], self.layer_dims[i + 1]\n",
    "            \n",
    "            # Xavier initialization for value network\n",
    "            limit = jnp.sqrt(6 / (fan_in + fan_out))\n",
    "            w = jax.random.uniform(subkey, (fan_in, fan_out), minval=-limit, maxval=limit)\n",
    "            b = jnp.zeros(fan_out)\n",
    "            \n",
    "            # Make the last layer even smaller to prevent extreme values\n",
    "            if i == len(self.layer_dims) - 2:  # Last layer\n",
    "                w *= 0.1  # Scale down output layer\n",
    "            \n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "        \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(state: jnp.ndarray, params: Tuple[List[jnp.ndarray], List[jnp.ndarray]]) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass returning state value estimate.\"\"\"\n",
    "        weights, biases = params\n",
    "        x = state\n",
    "        \n",
    "        # Hidden layers with ReLU activation\n",
    "        for w, b in zip(weights[:-1], biases[:-1]):\n",
    "            x = jax.nn.relu(x @ w + b)\n",
    "        \n",
    "        # Output layer (linear activation for value function)\n",
    "        value = x @ weights[-1] + biases[-1]\n",
    "        return value.squeeze()  # Remove extra dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing REINFORCE with Baseline for HalfCheetah:\n",
      "Discrete actions: [3 4 4 4 4 1]\n",
      "Continuous actions: [ 0.5  0.8  0.8  0.8  0.8 -0.5]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Discrete actions: [3 0 1 4 2 0]\n",
      "Continuous actions: [ 0.5 -0.8 -0.5  0.8  0.  -0.8]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Discrete actions: [1 0 0 1 2 1]\n",
      "Continuous actions: [-0.5 -0.8 -0.8 -0.5  0.  -0.5]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Discrete actions: [3 4 4 4 4 1]\n",
      "Continuous actions: [ 0.5  0.8  0.8  0.8  0.8 -0.5]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Discrete actions: [3 0 1 4 2 0]\n",
      "Continuous actions: [ 0.5 -0.8 -0.5  0.8  0.  -0.8]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Discrete actions: [1 0 0 1 2 1]\n",
      "Continuous actions: [-0.5 -0.8 -0.8 -0.5  0.  -0.5]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Average return: 0.7937\n",
      "Average advantage: 0.9311\n",
      "Average baseline: -0.1374\n",
      "Advantage std: 0.4030\n",
      "Baseline std: 0.0083\n",
      "Average return: 0.7937\n",
      "Average advantage: 0.9311\n",
      "Average baseline: -0.1374\n",
      "Advantage std: 0.4030\n",
      "Baseline std: 0.0083\n"
     ]
    }
   ],
   "source": [
    "class REINFORCE_Agent:\n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 policy_hidden_dims: List[int],\n",
    "                 value_hidden_dims: List[int], \n",
    "                 action_dim: int,\n",
    "                 discrete_bins: int = 5,\n",
    "                 learning_rate_theta: float = 1e-4,\n",
    "                 learning_rate_w: float = 5e-4,\n",
    "                 gamma: float = 0.99,\n",
    "                 gradient_clip: float = 1.0):\n",
    "        \"\"\"\n",
    "        REINFORCE agent with baseline for variance reduction.\n",
    "        Uses classical REINFORCE update rule with state-value function baseline.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: State space dimension\n",
    "            policy_hidden_dims: Hidden layer dimensions for policy network\n",
    "            value_hidden_dims: Hidden layer dimensions for value network\n",
    "            action_dim: Number of continuous action dimensions\n",
    "            discrete_bins: Number of discrete bins per action dimension\n",
    "            learning_rate_theta: Learning rate for policy parameters (α_θ)\n",
    "            learning_rate_w: Learning rate for value function parameters (α_w)\n",
    "            gamma: Discount factor\n",
    "            gradient_clip: Gradient clipping threshold\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.alpha_theta = learning_rate_theta\n",
    "        self.alpha_w = learning_rate_w\n",
    "        self.action_dim = action_dim\n",
    "        self.discrete_bins = discrete_bins\n",
    "        self.gradient_clip = gradient_clip\n",
    "        \n",
    "        # Initialize policy and value networks with different architectures\n",
    "        self.policy_network = PolicyNetwork(input_dim, policy_hidden_dims, action_dim, discrete_bins)\n",
    "        self.value_network = ValueNetwork(input_dim, value_hidden_dims)\n",
    "        \n",
    "        self.key = jax.random.PRNGKey(0)\n",
    "    \n",
    "    def select_action(self, state: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"Select discrete actions for each dimension, then convert to continuous.\"\"\"\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        \n",
    "        # Get action probabilities: shape (action_dim, discrete_bins)\n",
    "        action_probs = PolicyNetwork.forward(\n",
    "            state, self.policy_network.params, self.action_dim, self.discrete_bins\n",
    "        )\n",
    "        \n",
    "        # Sample discrete action for each dimension\n",
    "        keys = jax.random.split(subkey, self.action_dim)\n",
    "        discrete_actions = jnp.array([\n",
    "            jax.random.categorical(key, jnp.log(action_probs[i] + 1e-8))\n",
    "            for i, key in enumerate(keys)\n",
    "        ])\n",
    "        \n",
    "        # Convert discrete actions to continuous [-1, 1]\n",
    "        continuous_actions = self._discrete_to_continuous(discrete_actions)\n",
    "        \n",
    "        return discrete_actions, continuous_actions\n",
    "    \n",
    "    def _discrete_to_continuous(self, discrete_actions: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Convert discrete actions to continuous [-1, 1] range for HalfCheetah.\"\"\"\n",
    "        # Map discrete bins [0, 1, 2, 3, 4] to continuous values [-1, -0.5, 0, 0.5, 1]\n",
    "        continuous_actions = 2.0 * discrete_actions / (self.discrete_bins - 1) - 1.0\n",
    "        return jnp.clip(continuous_actions, -0.8, 0.8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_returns(rewards: jnp.ndarray, gamma: float) -> jnp.ndarray:\n",
    "        \"\"\"Compute discounted returns G_t for each time step.\"\"\"\n",
    "        returns = jnp.zeros_like(rewards)\n",
    "        G = 0.0\n",
    "        \n",
    "        # Compute returns backwards: G_t = R_{t+1} + γ*R_{t+2} + γ^2*R_{t+3} + ...\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            G = rewards[t] + gamma * G\n",
    "            returns = returns.at[t].set(G)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def policy_gradient(self, state: jnp.ndarray, action: jnp.ndarray) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Compute gradient of log π(a|s) with respect to policy parameters.\"\"\"\n",
    "        def log_prob_fn(params):\n",
    "            return PolicyNetwork.log_prob(state, action, params, self.action_dim, self.discrete_bins)\n",
    "        \n",
    "        grad_fn = jax.grad(log_prob_fn)\n",
    "        return grad_fn(self.policy_network.params)\n",
    "    \n",
    "    def value_gradient(self, state: jnp.ndarray, target: float) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Compute gradient of squared error loss for value function.\"\"\"\n",
    "        def value_loss_fn(params):\n",
    "            predicted_value = ValueNetwork.forward(state, params)\n",
    "            return 0.5 * (predicted_value - target) ** 2\n",
    "        \n",
    "        grad_fn = jax.grad(value_loss_fn)\n",
    "        return grad_fn(self.value_network.params)\n",
    "    \n",
    "    def update(self, states: jnp.ndarray, actions: jnp.ndarray, rewards: jnp.ndarray):\n",
    "        \"\"\"\n",
    "        REINFORCE with baseline update (proper episode-level batch updates):\n",
    "        1. Compute returns G_t for all steps\n",
    "        2. Compute advantages for all steps using current value network\n",
    "        3. Compute gradients for all steps\n",
    "        4. Apply batch updates to both networks\n",
    "        \"\"\"\n",
    "        # Compute returns G_t for each time step\n",
    "        returns = self.compute_returns(rewards, self.gamma)\n",
    "        \n",
    "        # Compute all advantages using current value network (before any updates)\n",
    "        advantages = []\n",
    "        baselines = []\n",
    "        for t in range(len(states)):\n",
    "            state_t = states[t]\n",
    "            return_t = returns[t]\n",
    "            baseline = ValueNetwork.forward(state_t, self.value_network.params)\n",
    "            advantage = return_t - baseline\n",
    "            advantages.append(float(advantage))\n",
    "            baselines.append(float(baseline))\n",
    "        \n",
    "        advantages = jnp.array(advantages)\n",
    "        \n",
    "        # Optional: Normalize advantages for stability (commented out for pure REINFORCE)\n",
    "        # advantage_mean = jnp.mean(advantages)\n",
    "        # advantage_std = jnp.std(advantages) + 1e-8\n",
    "        # advantages = (advantages - advantage_mean) / advantage_std\n",
    "        \n",
    "        # Initialize gradient accumulators\n",
    "        policy_weights, policy_biases = self.policy_network.params\n",
    "        value_weights, value_biases = self.value_network.params\n",
    "        \n",
    "        grad_policy_weights = [jnp.zeros_like(w) for w in policy_weights]\n",
    "        grad_policy_biases = [jnp.zeros_like(b) for b in policy_biases]\n",
    "        grad_value_weights = [jnp.zeros_like(w) for w in value_weights]\n",
    "        grad_value_biases = [jnp.zeros_like(b) for b in value_biases]\n",
    "        \n",
    "        # Accumulate gradients for all steps\n",
    "        for t in range(len(states)):\n",
    "            state_t = states[t]\n",
    "            action_t = actions[t]\n",
    "            return_t = returns[t]\n",
    "            advantage = advantages[t]\n",
    "            \n",
    "            # === Value Function Gradients (NOT scaled by advantage) ===\n",
    "            grad_v_w, grad_v_b = self.value_gradient(state_t, return_t)\n",
    "            for i in range(len(grad_value_weights)):\n",
    "                grad_value_weights[i] += grad_v_w[i]  # No advantage scaling here!\n",
    "                grad_value_biases[i] += grad_v_b[i]   # No advantage scaling here!\n",
    "            \n",
    "            # === Policy Gradients (scaled by advantage) ===\n",
    "            grad_p_w, grad_p_b = self.policy_gradient(state_t, action_t)\n",
    "            for i in range(len(grad_policy_weights)):\n",
    "                grad_policy_weights[i] += advantage * grad_p_w[i]\n",
    "                grad_policy_biases[i] += advantage * grad_p_b[i]\n",
    "        \n",
    "        # Apply gradient clipping and normalization\n",
    "        n_steps = len(states)\n",
    "        for i in range(len(grad_policy_weights)):\n",
    "            # Average gradients over episode\n",
    "            grad_policy_weights[i] /= n_steps\n",
    "            grad_policy_biases[i] /= n_steps\n",
    "            grad_value_weights[i] /= n_steps\n",
    "            grad_value_biases[i] /= n_steps\n",
    "            \n",
    "            # Clip gradients\n",
    "            grad_policy_weights[i] = jnp.clip(grad_policy_weights[i], -self.gradient_clip, self.gradient_clip)\n",
    "            grad_policy_biases[i] = jnp.clip(grad_policy_biases[i], -self.gradient_clip, self.gradient_clip)\n",
    "            grad_value_weights[i] = jnp.clip(grad_value_weights[i], -self.gradient_clip, self.gradient_clip)\n",
    "            grad_value_biases[i] = jnp.clip(grad_value_biases[i], -self.gradient_clip, self.gradient_clip)\n",
    "        \n",
    "        # Apply batch updates (single update per episode)\n",
    "        # Policy update: θ ← θ + α_θ * ∇_θ\n",
    "        new_policy_weights = [w + self.alpha_theta * gw for w, gw in zip(policy_weights, grad_policy_weights)]\n",
    "        new_policy_biases = [b + self.alpha_theta * gb for b, gb in zip(policy_biases, grad_policy_biases)]\n",
    "        \n",
    "        # Value function update: w ← w + α_w * ∇_w\n",
    "        new_value_weights = [w + self.alpha_w * gw for w, gw in zip(value_weights, grad_value_weights)]\n",
    "        new_value_biases = [b + self.alpha_w * gb for b, gb in zip(value_biases, grad_value_biases)]\n",
    "        \n",
    "        # Update parameters\n",
    "        self.policy_network.params = (new_policy_weights, new_policy_biases)\n",
    "        self.value_network.params = (new_value_weights, new_value_biases)\n",
    "        \n",
    "        # Return metrics for logging\n",
    "        avg_return = float(jnp.mean(returns))\n",
    "        avg_advantage = float(jnp.mean(advantages))\n",
    "        avg_baseline = float(jnp.mean(jnp.array(baselines)))\n",
    "        \n",
    "        # Check for NaNs and provide debugging info\n",
    "        if jnp.isnan(avg_advantage) or jnp.isnan(avg_baseline):\n",
    "            print(f\"WARNING: NaN detected!\")\n",
    "            print(f\"Returns: {returns}\")\n",
    "            print(f\"Baselines: {baselines}\")\n",
    "            print(f\"Advantages: {advantages}\")\n",
    "            # Return safe values\n",
    "            return 0.0, 0.0, 0.0\n",
    "        \n",
    "        advantage_std = jnp.std(advantages) + 1e-8\n",
    "        baseline_std = jnp.std(jnp.array(baselines)) + 1e-8\n",
    "        \n",
    "        return avg_return, avg_advantage, avg_baseline, advantage_std, baseline_std\n",
    "\n",
    "# Test the REINFORCE with baseline agent for HalfCheetah-v4\n",
    "agent = REINFORCE_Agent(\n",
    "    input_dim=17,                # HalfCheetah state space\n",
    "    policy_hidden_dims=[64, 64], \n",
    "    value_hidden_dims=[64, 32],\n",
    "    action_dim=6,                # HalfCheetah action space (6 joints)\n",
    "    discrete_bins=5,\n",
    "    learning_rate_theta=1e-4,    \n",
    "    learning_rate_w=5e-4,        \n",
    "    gamma=0.99,\n",
    "    gradient_clip=1.0            \n",
    ")\n",
    "\n",
    "# Test action selection\n",
    "state = jnp.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7])  # 17-dim state\n",
    "print(\"Testing REINFORCE with Baseline for HalfCheetah:\")\n",
    "for i in range(3):\n",
    "    discrete_actions, continuous_actions = agent.select_action(state)\n",
    "    baseline_value = ValueNetwork.forward(state, agent.value_network.params)\n",
    "    print(f\"Discrete actions: {discrete_actions}\")\n",
    "    print(f\"Continuous actions: {continuous_actions}\")\n",
    "    print(f\"Estimated state value: {baseline_value:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Test update with dummy data (17-dim states, 6-dim actions)\n",
    "dummy_states = jnp.array([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7],\n",
    "    [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8],\n",
    "    [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6]\n",
    "])\n",
    "dummy_actions = jnp.array([[0, 1, 2, 3, 4, 0], [1, 0, 3, 2, 1, 4], [2, 4, 1, 0, 3, 2]])  # 6-dim actions\n",
    "dummy_rewards = jnp.array([1.0, -0.5, 0.8])\n",
    "\n",
    "avg_return, avg_advantage, avg_baseline, advantage_std, baseline_std = agent.update(dummy_states, dummy_actions, dummy_rewards)\n",
    "print(f\"Average return: {avg_return:.4f}\")\n",
    "print(f\"Average advantage: {avg_advantage:.4f}\")\n",
    "print(f\"Average baseline: {avg_baseline:.4f}\")\n",
    "print(f\"Advantage std: {advantage_std:.4f}\")\n",
    "print(f\"Baseline std: {baseline_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting REINFORCE with baseline training...\n",
      "\n",
      "Environment: HalfCheetah-v5\n",
      "Observation space: 17\n",
      "Action space: 6 (continuous)\n",
      "Discrete bins per action: 7\n",
      "Action range: -1.0 to 1.0\n",
      "Rendering every 1 episodes\n",
      "Episode   1 | Reward: -138.09 | Return: -36.71 | Baseline:  -0.24 | Advantage: -36.47 | Advantage Std: 12.89 | Baseline Std: 0.25\n",
      "Episode   1 | Reward: -138.09 | Return: -36.71 | Baseline:  -0.24 | Advantage: -36.47 | Advantage Std: 12.89 | Baseline Std: 0.25\n",
      "Episode   2 | Reward: -75.84 | Return: -18.78 | Baseline:  -0.23 | Advantage: -18.55 | Advantage Std: 11.93 | Baseline Std: 0.23\n",
      "Episode   2 | Reward: -75.84 | Return: -18.78 | Baseline:  -0.23 | Advantage: -18.55 | Advantage Std: 11.93 | Baseline Std: 0.23\n",
      "Episode   3 | Reward: -98.09 | Return: -23.44 | Baseline:  -0.24 | Advantage: -23.20 | Advantage Std: 13.75 | Baseline Std: 0.27\n",
      "Episode   3 | Reward: -98.09 | Return: -23.44 | Baseline:  -0.24 | Advantage: -23.20 | Advantage Std: 13.75 | Baseline Std: 0.27\n",
      "Episode   4 | Reward: -64.43 | Return: -13.95 | Baseline:  -0.23 | Advantage: -13.71 | Advantage Std: 10.12 | Baseline Std: 0.24\n",
      "Episode   4 | Reward: -64.43 | Return: -13.95 | Baseline:  -0.23 | Advantage: -13.71 | Advantage Std: 10.12 | Baseline Std: 0.24\n",
      "Episode   5 | Reward: -86.32 | Return: -21.80 | Baseline:  -0.21 | Advantage: -21.59 | Advantage Std: 11.91 | Baseline Std: 0.24\n",
      "Episode   5 | Reward: -86.32 | Return: -21.80 | Baseline:  -0.21 | Advantage: -21.59 | Advantage Std: 11.91 | Baseline Std: 0.24\n",
      "Episode   6 | Reward:  -3.25 | Return:  -1.51 | Baseline:  -0.22 | Advantage:  -1.30 | Advantage Std: 7.30 | Baseline Std: 0.25\n",
      "Episode   6 | Reward:  -3.25 | Return:  -1.51 | Baseline:  -0.22 | Advantage:  -1.30 | Advantage Std: 7.30 | Baseline Std: 0.25\n",
      "Episode   7 | Reward: -109.34 | Return: -32.51 | Baseline:  -0.19 | Advantage: -32.32 | Advantage Std: 14.44 | Baseline Std: 0.24\n",
      "Episode   7 | Reward: -109.34 | Return: -32.51 | Baseline:  -0.19 | Advantage: -32.32 | Advantage Std: 14.44 | Baseline Std: 0.24\n",
      "Episode   8 | Reward: -19.83 | Return:  -7.25 | Baseline:  -0.21 | Advantage:  -7.04 | Advantage Std: 6.94 | Baseline Std: 0.25\n",
      "Episode   8 | Reward: -19.83 | Return:  -7.25 | Baseline:  -0.21 | Advantage:  -7.04 | Advantage Std: 6.94 | Baseline Std: 0.25\n",
      "Episode   9 | Reward: -39.08 | Return: -11.92 | Baseline:  -0.21 | Advantage: -11.71 | Advantage Std: 10.74 | Baseline Std: 0.24\n",
      "Episode   9 | Reward: -39.08 | Return: -11.92 | Baseline:  -0.21 | Advantage: -11.71 | Advantage Std: 10.74 | Baseline Std: 0.24\n",
      "Episode  10 | Reward: -122.05 | Return: -28.64 | Baseline:  -0.19 | Advantage: -28.45 | Advantage Std: 23.58 | Baseline Std: 0.23\n",
      "Episode  10 | Reward: -122.05 | Return: -28.64 | Baseline:  -0.19 | Advantage: -28.45 | Advantage Std: 23.58 | Baseline Std: 0.23\n",
      "Episode  11 | Reward:  43.97 | Return:  15.74 | Baseline:  -0.19 | Advantage:  15.94 | Advantage Std: 11.07 | Baseline Std: 0.27\n",
      "Episode  11 | Reward:  43.97 | Return:  15.74 | Baseline:  -0.19 | Advantage:  15.94 | Advantage Std: 11.07 | Baseline Std: 0.27\n",
      "Episode  12 | Reward: -70.94 | Return: -18.62 | Baseline:  -0.19 | Advantage: -18.43 | Advantage Std: 11.09 | Baseline Std: 0.25\n",
      "Episode  12 | Reward: -70.94 | Return: -18.62 | Baseline:  -0.19 | Advantage: -18.43 | Advantage Std: 11.09 | Baseline Std: 0.25\n",
      "Episode  13 | Reward: -157.27 | Return: -43.89 | Baseline:  -0.18 | Advantage: -43.71 | Advantage Std: 12.82 | Baseline Std: 0.24\n",
      "Episode  13 | Reward: -157.27 | Return: -43.89 | Baseline:  -0.18 | Advantage: -43.71 | Advantage Std: 12.82 | Baseline Std: 0.24\n",
      "Episode  14 | Reward:  16.87 | Return:   0.01 | Baseline:  -0.17 | Advantage:   0.18 | Advantage Std: 10.16 | Baseline Std: 0.23\n",
      "Episode  14 | Reward:  16.87 | Return:   0.01 | Baseline:  -0.17 | Advantage:   0.18 | Advantage Std: 10.16 | Baseline Std: 0.23\n",
      "Episode  15 | Reward: -99.28 | Return: -22.18 | Baseline:  -0.17 | Advantage: -22.01 | Advantage Std: 15.20 | Baseline Std: 0.22\n",
      "Episode  15 | Reward: -99.28 | Return: -22.18 | Baseline:  -0.17 | Advantage: -22.01 | Advantage Std: 15.20 | Baseline Std: 0.22\n",
      "Episode  16 | Reward: -11.29 | Return:  -2.84 | Baseline:  -0.18 | Advantage:  -2.66 | Advantage Std: 2.77 | Baseline Std: 0.25\n",
      "Episode  16 | Reward: -11.29 | Return:  -2.84 | Baseline:  -0.18 | Advantage:  -2.66 | Advantage Std: 2.77 | Baseline Std: 0.25\n",
      "Episode  17 | Reward: -51.59 | Return:  -9.03 | Baseline:  -0.15 | Advantage:  -8.88 | Advantage Std: 8.13 | Baseline Std: 0.21\n",
      "Episode  17 | Reward: -51.59 | Return:  -9.03 | Baseline:  -0.15 | Advantage:  -8.88 | Advantage Std: 8.13 | Baseline Std: 0.21\n",
      "Episode  18 | Reward:  20.99 | Return:   7.04 | Baseline:  -0.14 | Advantage:   7.18 | Advantage Std: 5.70 | Baseline Std: 0.23\n",
      "Episode  18 | Reward:  20.99 | Return:   7.04 | Baseline:  -0.14 | Advantage:   7.18 | Advantage Std: 5.70 | Baseline Std: 0.23\n",
      "Episode  19 | Reward: -153.19 | Return: -39.47 | Baseline:  -0.16 | Advantage: -39.31 | Advantage Std: 27.80 | Baseline Std: 0.22\n",
      "Episode  19 | Reward: -153.19 | Return: -39.47 | Baseline:  -0.16 | Advantage: -39.31 | Advantage Std: 27.80 | Baseline Std: 0.22\n",
      "Episode  20 | Reward: -91.06 | Return: -25.63 | Baseline:  -0.15 | Advantage: -25.49 | Advantage Std: 8.17 | Baseline Std: 0.22\n",
      "Episode  20 | Reward: -91.06 | Return: -25.63 | Baseline:  -0.15 | Advantage: -25.49 | Advantage Std: 8.17 | Baseline Std: 0.22\n",
      "Episode  21 | Reward: -116.23 | Return: -32.95 | Baseline:  -0.14 | Advantage: -32.81 | Advantage Std: 18.81 | Baseline Std: 0.22\n",
      "Episode  21 | Reward: -116.23 | Return: -32.95 | Baseline:  -0.14 | Advantage: -32.81 | Advantage Std: 18.81 | Baseline Std: 0.22\n",
      "Episode  22 | Reward: -46.42 | Return: -16.66 | Baseline:  -0.13 | Advantage: -16.52 | Advantage Std: 8.16 | Baseline Std: 0.23\n",
      "Episode  22 | Reward: -46.42 | Return: -16.66 | Baseline:  -0.13 | Advantage: -16.52 | Advantage Std: 8.16 | Baseline Std: 0.23\n",
      "Episode  23 | Reward: -18.75 | Return:  -3.90 | Baseline:  -0.14 | Advantage:  -3.76 | Advantage Std: 5.79 | Baseline Std: 0.23\n",
      "Episode  23 | Reward: -18.75 | Return:  -3.90 | Baseline:  -0.14 | Advantage:  -3.76 | Advantage Std: 5.79 | Baseline Std: 0.23\n",
      "\n",
      "Training interrupted by user.\n",
      "\n",
      "Training interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Training configuration for HalfCheetah-v4\n",
    "CONFIG = {\n",
    "    'gamma': 0.99,\n",
    "    'episode_length': 250,       # Shorter episodes for faster training\n",
    "    'episodes': 100000000000,            # Fewer episodes but more focused\n",
    "    'learning_rate_theta': 5e-4, # Higher learning rate for faster convergence\n",
    "    'learning_rate_w': 5e-4,     # Higher learning rate for value function\n",
    "    'policy_hidden_dims': [17, 32],    # Good network size for HalfCheetah\n",
    "    'value_hidden_dims': [17, 8],     # Smaller value network\n",
    "    'discrete_bins': 7,          # 5 bins per action dimension\n",
    "    'log_interval': 1,          # Log every 10 episodes\n",
    "    'render_interval': 1,       # Render every 50 episodes\n",
    "    'render_episodes': 1,\n",
    "    'render_episode_length': 200,\n",
    "    'gradient_clip': 1.0         # Standard gradient clipping\n",
    "}\n",
    "\n",
    "def run_demo_episode(agent, env_render, episode_num):\n",
    "    \"\"\"Run a single demonstration episode with rendering.\"\"\"\n",
    "    print(f\"\\n--- Demonstration Episode {episode_num} ---\")\n",
    "    observation, _ = env_render.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    for step in range(CONFIG['render_episode_length']):\n",
    "        # Convert observation to JAX array\n",
    "        state = jnp.array(observation, dtype=jnp.float32)\n",
    "        \n",
    "        # Select action using current policy\n",
    "        discrete_actions, continuous_actions = agent.select_action(state)\n",
    "        \n",
    "        # Take step in environment\n",
    "        observation, reward, terminated, truncated, _ = env_render.step(np.array(continuous_actions))\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        \n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"Demo episode reward: {total_reward:.2f} (steps: {step_count})\")\n",
    "    return total_reward\n",
    "\n",
    "def train_agent():\n",
    "    \"\"\"Train REINFORCE agent on HalfCheetah environment with periodic rendering.\"\"\"\n",
    "    # Training environment (no rendering for speed)\n",
    "    env = gym.make(\"HalfCheetah-v5\", render_mode='human')\n",
    "    \n",
    "    # Separate environment for rendering demonstrations\n",
    "    env_render = gym.make(\"HalfCheetah-v5\", render_mode=\"human\")\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    print(f\"Environment: HalfCheetah-v5\")\n",
    "    print(f\"Observation space: {obs_dim}\")\n",
    "    print(f\"Action space: {action_dim} (continuous)\")\n",
    "    print(f\"Discrete bins per action: {CONFIG['discrete_bins']}\")\n",
    "    print(f\"Action range: {env.action_space.low[0]:.1f} to {env.action_space.high[0]:.1f}\")\n",
    "    print(f\"Rendering every {CONFIG['render_interval']} episodes\")\n",
    "    \n",
    "    # Create agent with baseline\n",
    "    agent = REINFORCE_Agent(\n",
    "        input_dim=obs_dim,\n",
    "        policy_hidden_dims=CONFIG['policy_hidden_dims'],\n",
    "        value_hidden_dims=CONFIG['value_hidden_dims'],\n",
    "        action_dim=action_dim,\n",
    "        discrete_bins=CONFIG['discrete_bins'],\n",
    "        learning_rate_theta=CONFIG['learning_rate_theta'],\n",
    "        learning_rate_w=CONFIG['learning_rate_w'],\n",
    "        gamma=CONFIG['gamma'],\n",
    "        gradient_clip=CONFIG['gradient_clip']\n",
    "    )\n",
    "    \n",
    "    episode_rewards = []\n",
    "    demo_rewards = []\n",
    "    \n",
    "    for episode in range(CONFIG['episodes']):\n",
    "        observation, _ = env.reset()\n",
    "        \n",
    "        # Collect trajectory\n",
    "        states, discrete_actions_list, rewards = [], [], []\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(CONFIG['episode_length']):\n",
    "            # Convert observation to JAX array\n",
    "            state = jnp.array(observation, dtype=jnp.float32)\n",
    "            \n",
    "            # Select action (returns both discrete and continuous)\n",
    "            discrete_actions, continuous_actions = agent.select_action(state)\n",
    "            \n",
    "            # Store transition data\n",
    "            states.append(state)\n",
    "            discrete_actions_list.append(discrete_actions)\n",
    "            \n",
    "            # Take step in environment using continuous actions\n",
    "            observation, reward, terminated, truncated, _ = env.step(np.array(continuous_actions))\n",
    "            rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Convert to JAX arrays\n",
    "        states = jnp.stack(states)\n",
    "        discrete_actions_array = jnp.stack(discrete_actions_list)\n",
    "        rewards = jnp.array(rewards)\n",
    "        \n",
    "        # Update policy using REINFORCE with baseline\n",
    "        avg_return, avg_advantage, avg_baseline, advantage_std, baseline_std = agent.update(states, discrete_actions_array, rewards)\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Log progress\n",
    "        if (episode + 1) % CONFIG['log_interval'] == 0:\n",
    "            recent_reward = np.mean(episode_rewards[-CONFIG['log_interval']])\n",
    "            \n",
    "            print(f\"Episode {episode + 1:3d} | Reward: {recent_reward:6.2f} | Return: {avg_return:6.2f} | \"\n",
    "                  f\"Baseline: {avg_baseline:6.2f} | Advantage: {avg_advantage:6.2f} | \"\n",
    "                  f\"Advantage Std: {advantage_std:.2f} | Baseline Std: {baseline_std:.2f}\")\n",
    "        \n",
    "        # # Render demonstration episodes\n",
    "        # if (episode + 1) % CONFIG['render_interval'] == 0:\n",
    "        #     for demo_ep in range(CONFIG['render_episodes']):\n",
    "        #         demo_reward = run_demo_episode(agent, env_render, episode + 1)\n",
    "        #         demo_rewards.append(demo_reward)\n",
    "            \n",
    "    \n",
    "    env.close()\n",
    "    env_render.close()\n",
    "    \n",
    "    return episode_rewards, demo_rewards\n",
    "\n",
    "# Start training with rendering\n",
    "try:\n",
    "    print(\"Starting REINFORCE with baseline training...\\n\")\n",
    "    \n",
    "    training_rewards, demo_rewards = train_agent()\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Final average training reward: {np.mean(training_rewards[-10:]):.2f}\")\n",
    "    if demo_rewards:\n",
    "        print(f\"Final average demo reward: {np.mean(demo_rewards[-3:]):.2f}\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user.\")\n",
    "    #Close environments gracefully\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(\"This might be due to environment setup issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
