{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Agent for gymnasium environments\n",
    "In this notebook I present a very basic implementation of a REINFORCE agent for gymnasium environments. This is meant to work in continous observations and actions environments. The policy network is a multi-layer perceptron (MLP) that outputs a Gaussian distribution for the actions. The agent uses the REINFORCE algorithm to update the policy based on the rewards received from the environment.\n",
    "\n",
    "For the baseline I use a simple MLP that outputs a single value for the state, which is used to compute the advantage function. The agent is designed to work with environments that have continuous action spaces, such as Pendulum-v1.\n",
    "\n",
    "I used JAX as a framework for the implementation, which allows for efficient computation and automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for REINFORCE with Baseline implementation\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Environment and utilities\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork:\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], action_dim: int):\n",
    "        \"\"\"\n",
    "        Multi-layer policy network for continuous action spaces using Gaussian policy.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: State space dimension\n",
    "            hidden_dims: Hidden layer dimensions\n",
    "            action_dim: Number of continuous action dimensions (6 for HalfCheetah)\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Output size: 2 * action_dim (mean and log_std for each action)\n",
    "        self.output_dim = 2 * action_dim\n",
    "        self.layer_dims = [input_dim] + hidden_dims + [self.output_dim]\n",
    "        \n",
    "        self.params = self._initialize_params()\n",
    "    \n",
    "    def _initialize_params(self) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Initialize weights and biases using proper Xavier initialization.\"\"\"\n",
    "        key = jax.random.PRNGKey(42)\n",
    "        weights, biases = [], []\n",
    "        \n",
    "        for i in range(len(self.layer_dims) - 1):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            fan_in, fan_out = self.layer_dims[i], self.layer_dims[i + 1]\n",
    "            \n",
    "            # Proper Xavier initialization\n",
    "            limit = jnp.sqrt(6 / (fan_in + fan_out))\n",
    "            w = jax.random.uniform(subkey, (fan_in, fan_out), minval=-limit, maxval=limit)\n",
    "            b = jnp.zeros(fan_out)\n",
    "            \n",
    "            # Scale down the output layer for better initialization\n",
    "            if i == len(self.layer_dims) - 2:  # Output layer\n",
    "                w *= 0.1\n",
    "            \n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "        \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(state: jnp.ndarray, params: Tuple[List[jnp.ndarray], List[jnp.ndarray]], \n",
    "                action_dim: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"Forward pass returning action mean and log_std.\"\"\"\n",
    "        weights, biases = params\n",
    "        x = state\n",
    "        \n",
    "        # Hidden layers with ReLU activation\n",
    "        for w, b in zip(weights[:-1], biases[:-1]):\n",
    "            x = jax.nn.relu(x @ w + b)\n",
    "        \n",
    "        # Output layer\n",
    "        output = x @ weights[-1] + biases[-1]\n",
    "        \n",
    "        # Split output into mean and log_std\n",
    "        mean = output[:action_dim]\n",
    "        log_std = output[action_dim:] - 1.0  # Initialize log_std around -1 for smaller initial std\n",
    "        \n",
    "        # Clamp log_std to prevent extreme values\n",
    "        log_std = jnp.clip(log_std, -2.0, 2.0)  # std between ~0.135 and ~1.0\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_prob(state: jnp.ndarray, actions: jnp.ndarray, params: Tuple[List[jnp.ndarray], List[jnp.ndarray]],\n",
    "                action_dim: int) -> jnp.ndarray:\n",
    "        \"\"\"Compute log probability of actions under Gaussian policy.\"\"\"\n",
    "        mean, log_std = PolicyNetwork.forward(state, params, action_dim)\n",
    "        std = jnp.exp(log_std)\n",
    "        \n",
    "        # Gaussian log probability: log p(a) = -0.5 * ((a - μ) / σ)² - log(σ) - 0.5 * log(2π)\n",
    "        log_prob = -0.5 * jnp.sum(((actions - mean) / std) ** 2)\n",
    "        log_prob -= jnp.sum(log_std)  # -log(σ) terms\n",
    "        log_prob -= 0.5 * action_dim * jnp.log(2 * jnp.pi)  # constant term\n",
    "        \n",
    "        return log_prob\n",
    "\n",
    "\n",
    "class ValueNetwork:\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"\n",
    "        State-value function network for baseline estimation.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: State space dimension\n",
    "            hidden_dims: Hidden layer dimensions\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.layer_dims = [input_dim] + hidden_dims + [1]  # Output single value\n",
    "        \n",
    "        self.params = self._initialize_params()\n",
    "    \n",
    "    def _initialize_params(self) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Initialize weights and biases using Xavier initialization.\"\"\"\n",
    "        key = jax.random.PRNGKey(123)  # Different seed from policy network\n",
    "        weights, biases = [], []\n",
    "        \n",
    "        for i in range(len(self.layer_dims) - 1):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            fan_in, fan_out = self.layer_dims[i], self.layer_dims[i + 1]\n",
    "            \n",
    "            # Xavier initialization for value network\n",
    "            limit = jnp.sqrt(6 / (fan_in + fan_out))\n",
    "            w = jax.random.uniform(subkey, (fan_in, fan_out), minval=-limit, maxval=limit)\n",
    "            b = jnp.zeros(fan_out)\n",
    "            \n",
    "            # Make the last layer even smaller to prevent extreme values\n",
    "            if i == len(self.layer_dims) - 2:  # Last layer\n",
    "                w *= 0.1  # Scale down output layer\n",
    "            \n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "        \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(state: jnp.ndarray, params: Tuple[List[jnp.ndarray], List[jnp.ndarray]]) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass returning state value estimate.\"\"\"\n",
    "        weights, biases = params\n",
    "        x = state\n",
    "        \n",
    "        # Hidden layers with ReLU activation\n",
    "        for w, b in zip(weights[:-1], biases[:-1]):\n",
    "            x = jax.nn.relu(x @ w + b)\n",
    "        \n",
    "        # Output layer (linear activation for value function)\n",
    "        value = x @ weights[-1] + biases[-1]\n",
    "        return value.squeeze()  # Remove extra dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Simplified REINFORCE (Average Reward) for HalfCheetah:\n",
      "Action: [-0.55363524 -0.7422026  -0.50519276  0.44682175 -0.33735043 -0.52271694]\n",
      "Policy mean: [-0.03758024 -0.01273906  0.02240782  0.02175305  0.0345523  -0.03523846]\n",
      "Policy log_std: [-1.0387405  -1.0318427  -0.99308646 -1.0111479  -0.9646626  -0.9589811 ]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Action: [ 0.29218614 -0.01417697 -0.25183186 -0.4974694   0.4384162   0.3209747 ]\n",
      "Policy mean: [-0.03758024 -0.01273906  0.02240782  0.02175305  0.0345523  -0.03523846]\n",
      "Policy log_std: [-1.0387405  -1.0318427  -0.99308646 -1.0111479  -0.9646626  -0.9589811 ]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Action: [ 0.37598157  0.09857586 -0.18914562  0.07148103  0.3172931  -0.02049511]\n",
      "Policy mean: [-0.03758024 -0.01273906  0.02240782  0.02175305  0.0345523  -0.03523846]\n",
      "Policy log_std: [-1.0387405  -1.0318427  -0.99308646 -1.0111479  -0.9646626  -0.9589811 ]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Average reward: 0.4333\n",
      "Average advantage: 0.5707\n",
      "Average baseline: -0.1374\n",
      "Advantage std: 0.0083\n",
      "Baseline std: 0.0083\n",
      "Action mean: 0.0167, std: 0.1641, min: -0.3000, max: 0.3000\n"
     ]
    }
   ],
   "source": [
    "class REINFORCE_Agent:\n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 policy_hidden_dims: List[int],\n",
    "                 value_hidden_dims: List[int], \n",
    "                 action_dim: int,\n",
    "                 learning_rate_theta: float = 1e-4,\n",
    "                 learning_rate_w: float = 5e-4,\n",
    "                 gradient_clip: float = 1.0):\n",
    "        \"\"\"\n",
    "        Simplified REINFORCE agent that maximizes average reward per episode.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: State space dimension\n",
    "            policy_hidden_dims: Hidden layer dimensions for policy network\n",
    "            value_hidden_dims: Hidden layer dimensions for value network\n",
    "            action_dim: Number of continuous action dimensions\n",
    "            learning_rate_theta: Learning rate for policy parameters (α_θ)\n",
    "            learning_rate_w: Learning rate for value function parameters (α_w)\n",
    "            gradient_clip: Gradient clipping threshold\n",
    "        \"\"\"\n",
    "        self.alpha_theta = learning_rate_theta\n",
    "        self.alpha_w = learning_rate_w\n",
    "        self.action_dim = action_dim\n",
    "        self.gradient_clip = gradient_clip\n",
    "        \n",
    "        # Initialize policy and value networks\n",
    "        self.policy_network = PolicyNetwork(input_dim, policy_hidden_dims, action_dim)\n",
    "        self.value_network = ValueNetwork(input_dim, value_hidden_dims)\n",
    "        \n",
    "        self.key = jax.random.PRNGKey(0)\n",
    "    \n",
    "    def select_action(self, state: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Sample action from Gaussian policy.\"\"\"\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        \n",
    "        # Get mean and log_std from policy network\n",
    "        mean, log_std = PolicyNetwork.forward(state, self.policy_network.params, self.action_dim)\n",
    "        std = jnp.exp(log_std)\n",
    "        \n",
    "        # Sample from Gaussian distribution\n",
    "        action = mean + std * jax.random.normal(subkey, shape=(self.action_dim,))\n",
    "        \n",
    "        # Light clipping to environment bounds\n",
    "        action = jnp.clip(action, -3.0, 3.0)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def policy_gradient(self, state: jnp.ndarray, action: jnp.ndarray) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Compute gradient of log π(a|s) with respect to policy parameters.\"\"\"\n",
    "        def log_prob_fn(params):\n",
    "            return PolicyNetwork.log_prob(state, action, params, self.action_dim)\n",
    "        \n",
    "        grad_fn = jax.grad(log_prob_fn)\n",
    "        return grad_fn(self.policy_network.params)\n",
    "    \n",
    "    def value_gradient(self, state: jnp.ndarray, target: float) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Compute gradient of squared error loss for value function.\"\"\"\n",
    "        def value_loss_fn(params):\n",
    "            predicted_value = ValueNetwork.forward(state, params)\n",
    "            return 0.5 * (predicted_value - target) ** 2\n",
    "        \n",
    "        grad_fn = jax.grad(value_loss_fn)\n",
    "        return grad_fn(self.value_network.params)\n",
    "    \n",
    "    def update(self, states: jnp.ndarray, actions: jnp.ndarray, rewards: jnp.ndarray):\n",
    "        \"\"\"\n",
    "        Simple REINFORCE update that maximizes average reward per episode.\n",
    "        Uses the episode average reward as the target for all steps.\n",
    "        \"\"\"\n",
    "        # Compute average reward for this episode (simple and direct)\n",
    "        avg_reward = float(jnp.mean(rewards))\n",
    "        \n",
    "        # Compute baselines and advantages using current value network\n",
    "        advantages = []\n",
    "        baselines = []\n",
    "        for t in range(len(states)):\n",
    "            state_t = states[t]\n",
    "            baseline = ValueNetwork.forward(state_t, self.value_network.params)\n",
    "            advantage = avg_reward - baseline  # Use avg_reward instead of discounted return\n",
    "            advantages.append(float(advantage))\n",
    "            baselines.append(float(baseline))\n",
    "        \n",
    "        advantages = jnp.array(advantages)\n",
    "        \n",
    "        # Normalize advantages for stability\n",
    "        advantage_mean = jnp.mean(advantages)\n",
    "        advantage_std = jnp.std(advantages) + 1e-8\n",
    "        advantages = (advantages - advantage_mean) / advantage_std\n",
    "        \n",
    "        # Initialize gradient accumulators\n",
    "        policy_weights, policy_biases = self.policy_network.params\n",
    "        value_weights, value_biases = self.value_network.params\n",
    "        \n",
    "        grad_policy_weights = [jnp.zeros_like(w) for w in policy_weights]\n",
    "        grad_policy_biases = [jnp.zeros_like(b) for b in policy_biases]\n",
    "        grad_value_weights = [jnp.zeros_like(w) for w in value_weights]\n",
    "        grad_value_biases = [jnp.zeros_like(b) for b in value_biases]\n",
    "        \n",
    "        # Accumulate gradients for all steps\n",
    "        for t in range(len(states)):\n",
    "            state_t = states[t]\n",
    "            action_t = actions[t]\n",
    "            advantage = advantages[t]\n",
    "            \n",
    "            # Value function gradients (target is average reward)\n",
    "            grad_v_w, grad_v_b = self.value_gradient(state_t, avg_reward)\n",
    "            for i in range(len(grad_value_weights)):\n",
    "                grad_value_weights[i] += grad_v_w[i]\n",
    "                grad_value_biases[i] += grad_v_b[i]\n",
    "            \n",
    "            # Policy gradients (scaled by advantage)\n",
    "            grad_p_w, grad_p_b = self.policy_gradient(state_t, action_t)\n",
    "            for i in range(len(grad_policy_weights)):\n",
    "                grad_policy_weights[i] += advantage * grad_p_w[i]\n",
    "                grad_policy_biases[i] += advantage * grad_p_b[i]\n",
    "        \n",
    "        # Apply gradient clipping and normalization\n",
    "        n_steps = len(states)\n",
    "        for i in range(len(grad_policy_weights)):\n",
    "            # Average gradients over episode\n",
    "            grad_policy_weights[i] /= n_steps\n",
    "            grad_policy_biases[i] /= n_steps\n",
    "            grad_value_weights[i] /= n_steps\n",
    "            grad_value_biases[i] /= n_steps\n",
    "            \n",
    "            # Clip gradients\n",
    "            grad_policy_weights[i] = jnp.clip(grad_policy_weights[i], -self.gradient_clip, self.gradient_clip)\n",
    "            grad_policy_biases[i] = jnp.clip(grad_policy_biases[i], -self.gradient_clip, self.gradient_clip)\n",
    "            grad_value_weights[i] = jnp.clip(grad_value_weights[i], -self.gradient_clip, self.gradient_clip)\n",
    "            grad_value_biases[i] = jnp.clip(grad_value_biases[i], -self.gradient_clip, self.gradient_clip)\n",
    "        \n",
    "        # Apply batch updates\n",
    "        new_policy_weights = [w + self.alpha_theta * gw for w, gw in zip(policy_weights, grad_policy_weights)]\n",
    "        new_policy_biases = [b + self.alpha_theta * gb for b, gb in zip(policy_biases, grad_policy_biases)]\n",
    "        new_value_weights = [w + self.alpha_w * gw for w, gw in zip(value_weights, grad_value_weights)]\n",
    "        new_value_biases = [b + self.alpha_w * gb for b, gb in zip(value_biases, grad_value_biases)]\n",
    "        \n",
    "        # Update parameters\n",
    "        self.policy_network.params = (new_policy_weights, new_policy_biases)\n",
    "        self.value_network.params = (new_value_weights, new_value_biases)\n",
    "        \n",
    "        # Compute training metrics (using original advantages for logging)\n",
    "        orig_advantages = jnp.array([float(avg_reward - baselines[t]) for t in range(len(states))])\n",
    "        avg_advantage = float(jnp.mean(orig_advantages))\n",
    "        avg_baseline = float(jnp.mean(jnp.array(baselines)))\n",
    "        advantage_std = float(jnp.std(orig_advantages) + 1e-8)\n",
    "        baseline_std = float(jnp.std(jnp.array(baselines)) + 1e-8)\n",
    "        \n",
    "        # Compute action statistics for logging\n",
    "        action_mean = float(jnp.mean(actions))\n",
    "        action_std = float(jnp.std(actions) + 1e-8)\n",
    "        action_min = float(jnp.min(actions))\n",
    "        action_max = float(jnp.max(actions))\n",
    "        \n",
    "        # Check for NaNs\n",
    "        if jnp.isnan(avg_advantage) or jnp.isnan(avg_baseline):\n",
    "            print(f\"WARNING: NaN detected!\")\n",
    "            return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        \n",
    "        return avg_reward, avg_advantage, avg_baseline, advantage_std, baseline_std, action_mean, action_std, action_min, action_max\n",
    "\n",
    "# Test the simplified REINFORCE for average reward maximization\n",
    "agent = REINFORCE_Agent(\n",
    "    input_dim=17,\n",
    "    policy_hidden_dims=[64, 64], \n",
    "    value_hidden_dims=[64, 32],\n",
    "    action_dim=6,\n",
    "    learning_rate_theta=1e-4,    \n",
    "    learning_rate_w=5e-4,        \n",
    "    gradient_clip=1.0            \n",
    ")\n",
    "\n",
    "# Test Gaussian action selection\n",
    "state = jnp.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7])  # 17-dim state\n",
    "print(\"Testing Simplified REINFORCE (Average Reward) for HalfCheetah:\")\n",
    "for i in range(3):\n",
    "    action = agent.select_action(state)\n",
    "    baseline_value = ValueNetwork.forward(state, agent.value_network.params)\n",
    "    mean, log_std = PolicyNetwork.forward(state, agent.policy_network.params, agent.action_dim)\n",
    "    print(f\"Action: {action}\")\n",
    "    print(f\"Policy mean: {mean}\")\n",
    "    print(f\"Policy log_std: {log_std}\")\n",
    "    print(f\"Estimated state value: {baseline_value:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Test update with dummy data\n",
    "dummy_states = jnp.array([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7],\n",
    "    [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8],\n",
    "    [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6]\n",
    "])\n",
    "dummy_actions = jnp.array([[0.1, -0.2, 0.3, -0.1, 0.0, 0.2], [0.0, 0.1, -0.3, 0.2, -0.1, 0.0], [-0.1, 0.0, 0.1, 0.0, 0.3, -0.2]])  # Continuous actions\n",
    "dummy_rewards = jnp.array([1.0, -0.5, 0.8])\n",
    "\n",
    "result = agent.update(dummy_states, dummy_actions, dummy_rewards)\n",
    "avg_reward, avg_advantage, avg_baseline, advantage_std, baseline_std, action_mean, action_std, action_min, action_max = result\n",
    "\n",
    "print(f\"Average reward: {avg_reward:.4f}\")\n",
    "print(f\"Average advantage: {avg_advantage:.4f}\")\n",
    "print(f\"Average baseline: {avg_baseline:.4f}\")\n",
    "print(f\"Advantage std: {advantage_std:.4f}\")\n",
    "print(f\"Baseline std: {baseline_std:.4f}\")\n",
    "print(f\"Action mean: {action_mean:.4f}, std: {action_std:.4f}, min: {action_min:.4f}, max: {action_max:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING ON SIMPLE ENVIRONMENT: Pendulum-v1\n",
      "============================================================\n",
      "Environment: Pendulum-v1\n",
      "Observation space: 3\n",
      "Action space: 1 (continuous)\n",
      "Action range: -2.0 to 2.0\n",
      "Training for 1000000 episodes with 100 steps each\n",
      "Rendering every 10 episodes\n",
      "\n",
      "Ep   1 | Reward:   496.1 | AvgRew:    5.0 | Base:  -0.2 | Adv:    5.2 | ActMean:  0.07, Std: 0.46\n",
      "Ep   2 | Reward:   217.4 | AvgRew:    2.2 | Base:  -0.1 | Adv:    2.3 | ActMean:  0.05, Std: 0.39\n",
      "Ep   3 | Reward:   298.8 | AvgRew:    3.0 | Base:  -0.2 | Adv:    3.1 | ActMean:  0.02, Std: 0.38\n",
      "Ep   4 | Reward:   249.0 | AvgRew:    2.5 | Base:  -0.1 | Adv:    2.6 | ActMean:  0.03, Std: 0.36\n",
      "Ep   5 | Reward:   240.9 | AvgRew:    2.4 | Base:  -0.1 | Adv:    2.5 | ActMean: -0.00, Std: 0.38\n",
      "Ep   6 | Reward:   250.6 | AvgRew:    2.5 | Base:  -0.2 | Adv:    2.7 | ActMean:  0.04, Std: 0.35\n",
      "Ep   7 | Reward:   240.8 | AvgRew:    2.4 | Base:  -0.2 | Adv:    2.6 | ActMean:  0.08, Std: 0.35\n",
      "Ep   8 | Reward:   514.1 | AvgRew:    5.1 | Base:  -0.3 | Adv:    5.5 | ActMean:  0.00, Std: 0.40\n",
      "Ep   9 | Reward:   412.5 | AvgRew:    4.1 | Base:  -0.3 | Adv:    4.4 | ActMean:  0.05, Std: 0.41\n",
      "Ep  10 | Reward:   165.2 | AvgRew:    1.7 | Base:  -0.2 | Adv:    1.8 | ActMean:  0.00, Std: 0.38\n",
      "  --> Episode 10 was rendered (Reward: 165.2)\n",
      "Ep  11 | Reward:   613.5 | AvgRew:    6.1 | Base:  -0.4 | Adv:    6.5 | ActMean:  0.08, Std: 0.46\n",
      "Ep  12 | Reward:   214.6 | AvgRew:    2.1 | Base:  -0.2 | Adv:    2.3 | ActMean:  0.07, Std: 0.39\n",
      "Ep  13 | Reward:   419.7 | AvgRew:    4.2 | Base:  -0.4 | Adv:    4.5 | ActMean:  0.04, Std: 0.37\n",
      "Ep  14 | Reward:    90.6 | AvgRew:    0.9 | Base:  -0.2 | Adv:    1.1 | ActMean:  0.01, Std: 0.35\n",
      "Ep  15 | Reward:   394.8 | AvgRew:    3.9 | Base:  -0.3 | Adv:    4.3 | ActMean: -0.01, Std: 0.34\n",
      "Ep  16 | Reward:   249.1 | AvgRew:    2.5 | Base:  -0.3 | Adv:    2.8 | ActMean:  0.05, Std: 0.33\n",
      "Ep  17 | Reward:   616.8 | AvgRew:    6.2 | Base:  -0.5 | Adv:    6.7 | ActMean:  0.09, Std: 0.46\n",
      "Ep  18 | Reward:   599.3 | AvgRew:    6.0 | Base:  -0.5 | Adv:    6.5 | ActMean:  0.13, Std: 0.42\n",
      "\n",
      "Training interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "# Test on a simpler environment: Pendulum-v1\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Simple configuration for Pendulum\n",
    "SIMPLE_CONFIG = {\n",
    "    'episode_length': 100,  # The length of each episode\n",
    "    'episodes': 1000,   # The number of episodes to train\n",
    "    'learning_rate_theta': 3e-3,    # Learning rate for policy parameters (α_θ)\n",
    "    'learning_rate_w': 3e-3,      # Learning rate for value function parameters (α_w)\n",
    "    'policy_hidden_dims': [8, 8],   # Hidden layers for policy network \n",
    "    'value_hidden_dims': [8, 8],    # Hidden layers for value network\n",
    "    'log_interval': 1,  # Every how many episodes to log\n",
    "    'render_interval': 10,  # Every how many episodes to render the environment\n",
    "    'gradient_clip': 1.0    # Gradient clipping threshold\n",
    "}\n",
    "\n",
    "def train_simple_env():\n",
    "    \"\"\"Train REINFORCE agent on Pendulum-v1\"\"\"\n",
    "    # Create training environment (no rendering)\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    # Create rendering environment for periodic demos\n",
    "    env_render = gym.make(\"Pendulum-v1\", render_mode='human')\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]  # 3 for Pendulum\n",
    "    action_dim = env.action_space.shape[0]    # 1 for Pendulum\n",
    "    \n",
    "    print(f\"Environment: Pendulum-v1\")\n",
    "    print(f\"Observation space: {obs_dim}\")\n",
    "    print(f\"Action space: {action_dim} (continuous)\")\n",
    "    print(f\"Action range: {env.action_space.low[0]:.1f} to {env.action_space.high[0]:.1f}\")\n",
    "    print(f\"Training for {SIMPLE_CONFIG['episodes']} episodes with {SIMPLE_CONFIG['episode_length']} steps each\")\n",
    "    print(f\"Rendering every {SIMPLE_CONFIG['render_interval']} episodes\\n\")\n",
    "    \n",
    "    # Create simple agent\n",
    "    agent = REINFORCE_Agent(\n",
    "        input_dim=obs_dim,\n",
    "        policy_hidden_dims=SIMPLE_CONFIG['policy_hidden_dims'],\n",
    "        value_hidden_dims=SIMPLE_CONFIG['value_hidden_dims'],\n",
    "        action_dim=action_dim,\n",
    "        learning_rate_theta=SIMPLE_CONFIG['learning_rate_theta'],\n",
    "        learning_rate_w=SIMPLE_CONFIG['learning_rate_w'],\n",
    "        gradient_clip=SIMPLE_CONFIG['gradient_clip']\n",
    "    )\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(SIMPLE_CONFIG['episodes']):\n",
    "        # Choose environment (render or not)\n",
    "        current_env = env_render if (episode + 1) % SIMPLE_CONFIG['render_interval'] == 0 else env\n",
    "        \n",
    "        observation, _ = current_env.reset()\n",
    "        \n",
    "        # Collect trajectory\n",
    "        states, actions_list, rewards = [], [], []\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(SIMPLE_CONFIG['episode_length']):\n",
    "            # Convert observation to JAX array\n",
    "            state = jnp.array(observation, dtype=jnp.float32)\n",
    "            \n",
    "            # Select action using Gaussian policy\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Store transition data\n",
    "            states.append(state)\n",
    "            actions_list.append(action)\n",
    "            \n",
    "            # Take step in environment\n",
    "            observation, reward, terminated, truncated, _ = current_env.step(np.array(action))\n",
    "            reward += 10\n",
    "            # Don't modify the reward - use original Pendulum rewards\n",
    "            rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Convert to JAX arrays\n",
    "        states = jnp.stack(states)\n",
    "        actions = jnp.stack(actions_list)\n",
    "        rewards = jnp.array(rewards)\n",
    "        \n",
    "        # Update policy using REINFORCE (maximizing average reward)\n",
    "        result = agent.update(states, actions, rewards)\n",
    "        avg_reward, avg_advantage, avg_baseline, advantage_std, baseline_std, action_mean, action_std, action_min, action_max = result\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Log progress (fix duplicate logging)\n",
    "        if (episode + 1) % SIMPLE_CONFIG['log_interval'] == 0:\n",
    "            recent_reward = np.mean(episode_rewards[-SIMPLE_CONFIG['log_interval']])\n",
    "            \n",
    "            print(f\"Ep {episode + 1:3d} | Reward: {recent_reward:7.1f} | AvgRew: {avg_reward:6.1f} | Base: {avg_baseline:5.1f} | Adv: {avg_advantage:6.1f} | ActMean: {action_mean:5.2f}, Std: {action_std:4.2f}\")\n",
    "            \n",
    "            # Special message for rendered episodes (only once)\n",
    "            if (episode + 1) % SIMPLE_CONFIG['render_interval'] == 0:\n",
    "                print(f\"  --> Episode {episode + 1} was rendered (Reward: {total_reward:.1f})\")\n",
    "    \n",
    "    env.close()\n",
    "    env_render.close()\n",
    "    return episode_rewards\n",
    "\n",
    "# Run training on simple environment\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING ON SIMPLE ENVIRONMENT: Pendulum-v1\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    simple_rewards = train_simple_env()\n",
    "    \n",
    "    print(f\"\\nSimple environment training completed!\")\n",
    "    print(f\"Final 10-episode average reward: {np.mean(simple_rewards[-10:]):.1f}\")\n",
    "    print(f\"Best 10-episode average reward: {max([np.mean(simple_rewards[i:i+10]) for i in range(len(simple_rewards)-9)]):.1f}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
