{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for REINFORCE with Baseline implementation\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Environment and utilities\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork:\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], action_dim: int):\n",
    "        \"\"\"\n",
    "        Multi-layer policy network for continuous action spaces using Gaussian policy.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: State space dimension\n",
    "            hidden_dims: Hidden layer dimensions\n",
    "            action_dim: Number of continuous action dimensions (6 for HalfCheetah)\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Output size: 2 * action_dim (mean and log_std for each action)\n",
    "        self.output_dim = 2 * action_dim\n",
    "        self.layer_dims = [input_dim] + hidden_dims + [self.output_dim]\n",
    "        \n",
    "        self.params = self._initialize_params()\n",
    "    \n",
    "    def _initialize_params(self) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Initialize weights and biases using Xavier initialization.\"\"\"\n",
    "        key = jax.random.PRNGKey(42)\n",
    "        weights, biases = [], []\n",
    "        \n",
    "        for i in range(len(self.layer_dims) - 1):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            fan_in, fan_out = self.layer_dims[i], self.layer_dims[i + 1]\n",
    "            \n",
    "            # Xavier initialization\n",
    "            limit = jnp.sqrt(6 / (fan_in + fan_out))\n",
    "            w = jnp.zeros((fan_in, fan_out))\n",
    "            b = jnp.zeros(fan_out)\n",
    "            \n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "        \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(state: jnp.ndarray, params: Tuple[List[jnp.ndarray], List[jnp.ndarray]], \n",
    "                action_dim: int) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"Forward pass returning action mean and log_std.\"\"\"\n",
    "        weights, biases = params\n",
    "        x = state\n",
    "        \n",
    "        # Hidden layers with ReLU activation\n",
    "        for w, b in zip(weights[:-1], biases[:-1]):\n",
    "            x = jax.nn.relu(x @ w + b)\n",
    "        \n",
    "        # Output layer\n",
    "        output = x @ weights[-1] + biases[-1]\n",
    "        \n",
    "        # Split output into mean and log_std\n",
    "        mean = output[:action_dim]\n",
    "        log_std = output[action_dim:] - 0.5  # Center log_std around 0.5 for better exploration\n",
    "        \n",
    "        # Clamp log_std to prevent extreme values\n",
    "        log_std = jnp.clip(log_std, -2.0, 0.5)  # std between ~0.135 and ~1.65\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_prob(state: jnp.ndarray, actions: jnp.ndarray, params: Tuple[List[jnp.ndarray], List[jnp.ndarray]],\n",
    "                action_dim: int) -> jnp.ndarray:\n",
    "        \"\"\"Compute log probability of actions under Gaussian policy.\"\"\"\n",
    "        mean, log_std = PolicyNetwork.forward(state, params, action_dim)\n",
    "        std = jnp.exp(log_std)\n",
    "        \n",
    "        # Gaussian log probability: log p(a) = -0.5 * ((a - μ) / σ)² - log(σ) - 0.5 * log(2π)\n",
    "        log_prob = -0.5 * jnp.sum(((actions - mean) / std) ** 2)\n",
    "        log_prob -= jnp.sum(log_std)  # -log(σ) terms\n",
    "        log_prob -= 0.5 * action_dim * jnp.log(2 * jnp.pi)  # constant term\n",
    "        \n",
    "        return log_prob\n",
    "\n",
    "\n",
    "class ValueNetwork:\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int]):\n",
    "        \"\"\"\n",
    "        State-value function network for baseline estimation.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: State space dimension\n",
    "            hidden_dims: Hidden layer dimensions\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.layer_dims = [input_dim] + hidden_dims + [1]  # Output single value\n",
    "        \n",
    "        self.params = self._initialize_params()\n",
    "    \n",
    "    def _initialize_params(self) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Initialize weights and biases using Xavier initialization.\"\"\"\n",
    "        key = jax.random.PRNGKey(123)  # Different seed from policy network\n",
    "        weights, biases = [], []\n",
    "        \n",
    "        for i in range(len(self.layer_dims) - 1):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            fan_in, fan_out = self.layer_dims[i], self.layer_dims[i + 1]\n",
    "            \n",
    "            # Xavier initialization for value network\n",
    "            limit = jnp.sqrt(6 / (fan_in + fan_out))\n",
    "            w = jax.random.uniform(subkey, (fan_in, fan_out), minval=-limit, maxval=limit)\n",
    "            b = jnp.zeros(fan_out)\n",
    "            \n",
    "            # Make the last layer even smaller to prevent extreme values\n",
    "            if i == len(self.layer_dims) - 2:  # Last layer\n",
    "                w *= 0.1  # Scale down output layer\n",
    "            \n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "        \n",
    "        return weights, biases\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(state: jnp.ndarray, params: Tuple[List[jnp.ndarray], List[jnp.ndarray]]) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass returning state value estimate.\"\"\"\n",
    "        weights, biases = params\n",
    "        x = state\n",
    "        \n",
    "        # Hidden layers with ReLU activation\n",
    "        for w, b in zip(weights[:-1], biases[:-1]):\n",
    "            x = jax.nn.relu(x @ w + b)\n",
    "        \n",
    "        # Output layer (linear activation for value function)\n",
    "        value = x @ weights[-1] + biases[-1]\n",
    "        return value.squeeze()  # Remove extra dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing REINFORCE with Gaussian Policy for HalfCheetah:\n",
      "Action: [-0.8844393  -1.241595   -0.86387324  0.70867616 -0.5918747  -0.77141565]\n",
      "Policy mean: [0. 0. 0. 0. 0. 0.]\n",
      "Policy log_std: [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Action: [ 0.56516916 -0.00244742 -0.44902968 -0.8656496   0.6427402   0.5636934 ]\n",
      "Policy mean: [0. 0. 0. 0. 0. 0.]\n",
      "Policy log_std: [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Action: [ 0.70878166  0.18946531 -0.3463896   0.08290668  0.44997555  0.02333077]\n",
      "Policy mean: [0. 0. 0. 0. 0. 0.]\n",
      "Policy log_std: [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Action: [-0.8844393  -1.241595   -0.86387324  0.70867616 -0.5918747  -0.77141565]\n",
      "Policy mean: [0. 0. 0. 0. 0. 0.]\n",
      "Policy log_std: [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Action: [ 0.56516916 -0.00244742 -0.44902968 -0.8656496   0.6427402   0.5636934 ]\n",
      "Policy mean: [0. 0. 0. 0. 0. 0.]\n",
      "Policy log_std: [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Action: [ 0.70878166  0.18946531 -0.3463896   0.08290668  0.44997555  0.02333077]\n",
      "Policy mean: [0. 0. 0. 0. 0. 0.]\n",
      "Policy log_std: [-0.5 -0.5 -0.5 -0.5 -0.5 -0.5]\n",
      "Estimated state value: -0.1375\n",
      "\n",
      "Average return: 0.7937\n",
      "Average advantage: 0.9311\n",
      "Average baseline: -0.1374\n",
      "Advantage std: 0.4030\n",
      "Baseline std: 0.0083\n",
      "Action mean: 0.0167, std: 0.1641, min: -0.3000, max: 0.3000\n",
      "Average return: 0.7937\n",
      "Average advantage: 0.9311\n",
      "Average baseline: -0.1374\n",
      "Advantage std: 0.4030\n",
      "Baseline std: 0.0083\n",
      "Action mean: 0.0167, std: 0.1641, min: -0.3000, max: 0.3000\n"
     ]
    }
   ],
   "source": [
    "class REINFORCE_Agent:\n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 policy_hidden_dims: List[int],\n",
    "                 value_hidden_dims: List[int], \n",
    "                 action_dim: int,\n",
    "                 learning_rate_theta: float = 1e-4,\n",
    "                 learning_rate_w: float = 5e-4,\n",
    "                 gamma: float = 0.99,\n",
    "                 gradient_clip: float = 1.0):\n",
    "        \"\"\"\n",
    "        REINFORCE agent with baseline for continuous control using Gaussian policy.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: State space dimension\n",
    "            policy_hidden_dims: Hidden layer dimensions for policy network\n",
    "            value_hidden_dims: Hidden layer dimensions for value network\n",
    "            action_dim: Number of continuous action dimensions\n",
    "            learning_rate_theta: Learning rate for policy parameters (α_θ)\n",
    "            learning_rate_w: Learning rate for value function parameters (α_w)\n",
    "            gamma: Discount factor\n",
    "            gradient_clip: Gradient clipping threshold\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.alpha_theta = learning_rate_theta\n",
    "        self.alpha_w = learning_rate_w\n",
    "        self.action_dim = action_dim\n",
    "        self.gradient_clip = gradient_clip\n",
    "        \n",
    "        # Initialize policy and value networks\n",
    "        self.policy_network = PolicyNetwork(input_dim, policy_hidden_dims, action_dim)\n",
    "        self.value_network = ValueNetwork(input_dim, value_hidden_dims)\n",
    "        \n",
    "        self.key = jax.random.PRNGKey(0)\n",
    "    \n",
    "    def select_action(self, state: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Sample action from Gaussian policy.\"\"\"\n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        \n",
    "        # Get mean and log_std from policy network\n",
    "        mean, log_std = PolicyNetwork.forward(state, self.policy_network.params, self.action_dim)\n",
    "        std = jnp.exp(log_std)\n",
    "        \n",
    "        # Sample from Gaussian distribution\n",
    "        action = mean + std * jax.random.normal(subkey, shape=(self.action_dim,))\n",
    "        \n",
    "        # Light clipping to environment bounds (less aggressive)\n",
    "        action = jnp.clip(action, -3.0, 3.0)  # Reduced clipping range\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_returns(rewards: jnp.ndarray, gamma: float) -> jnp.ndarray:\n",
    "        \"\"\"Compute discounted returns G_t for each time step.\"\"\"\n",
    "        returns = jnp.zeros_like(rewards)\n",
    "        G = 0.0\n",
    "        \n",
    "        # Compute returns backwards: G_t = R_{t+1} + γ*R_{t+2} + γ^2*R_{t+3} + ...\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            G = rewards[t] + gamma * G\n",
    "            returns = returns.at[t].set(G)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def policy_gradient(self, state: jnp.ndarray, action: jnp.ndarray) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Compute gradient of log π(a|s) with respect to policy parameters.\"\"\"\n",
    "        def log_prob_fn(params):\n",
    "            return PolicyNetwork.log_prob(state, action, params, self.action_dim)\n",
    "        \n",
    "        grad_fn = jax.grad(log_prob_fn)\n",
    "        return grad_fn(self.policy_network.params)\n",
    "    \n",
    "    def value_gradient(self, state: jnp.ndarray, target: float) -> Tuple[List[jnp.ndarray], List[jnp.ndarray]]:\n",
    "        \"\"\"Compute gradient of squared error loss for value function.\"\"\"\n",
    "        def value_loss_fn(params):\n",
    "            predicted_value = ValueNetwork.forward(state, params)\n",
    "            return 0.5 * (predicted_value - target) ** 2\n",
    "        \n",
    "        grad_fn = jax.grad(value_loss_fn)\n",
    "        return grad_fn(self.value_network.params)\n",
    "    \n",
    "    def update(self, states: jnp.ndarray, actions: jnp.ndarray, rewards: jnp.ndarray):\n",
    "        \"\"\"\n",
    "        REINFORCE with baseline update using Gaussian policy.\n",
    "        Returns training metrics and action statistics.\n",
    "        \"\"\"\n",
    "        # Compute returns G_t for each time step\n",
    "        returns = self.compute_returns(rewards, self.gamma)\n",
    "        \n",
    "        # Compute all advantages using current value network\n",
    "        advantages = []\n",
    "        baselines = []\n",
    "        for t in range(len(states)):\n",
    "            state_t = states[t]\n",
    "            return_t = returns[t]\n",
    "            baseline = ValueNetwork.forward(state_t, self.value_network.params)\n",
    "            advantage = return_t - baseline\n",
    "            advantages.append(float(advantage))\n",
    "            baselines.append(float(baseline))\n",
    "        \n",
    "        advantages = jnp.array(advantages)\n",
    "        \n",
    "        # Normalize advantages for stability (ENABLED)\n",
    "        advantage_mean = jnp.mean(advantages)\n",
    "        advantage_std = jnp.std(advantages) + 1e-8\n",
    "        advantages = (advantages - advantage_mean) / advantage_std\n",
    "        \n",
    "        # Initialize gradient accumulators\n",
    "        policy_weights, policy_biases = self.policy_network.params\n",
    "        value_weights, value_biases = self.value_network.params\n",
    "        \n",
    "        grad_policy_weights = [jnp.zeros_like(w) for w in policy_weights]\n",
    "        grad_policy_biases = [jnp.zeros_like(b) for b in policy_biases]\n",
    "        grad_value_weights = [jnp.zeros_like(w) for w in value_weights]\n",
    "        grad_value_biases = [jnp.zeros_like(b) for b in value_biases]\n",
    "        \n",
    "        # Accumulate gradients for all steps\n",
    "        for t in range(len(states)):\n",
    "            state_t = states[t]\n",
    "            action_t = actions[t]\n",
    "            return_t = returns[t]\n",
    "            advantage = advantages[t]\n",
    "            \n",
    "            # Value function gradients\n",
    "            grad_v_w, grad_v_b = self.value_gradient(state_t, return_t)\n",
    "            for i in range(len(grad_value_weights)):\n",
    "                grad_value_weights[i] += grad_v_w[i]\n",
    "                grad_value_biases[i] += grad_v_b[i]\n",
    "            \n",
    "            # Policy gradients (scaled by advantage)\n",
    "            grad_p_w, grad_p_b = self.policy_gradient(state_t, action_t)\n",
    "            for i in range(len(grad_policy_weights)):\n",
    "                grad_policy_weights[i] += advantage * grad_p_w[i]\n",
    "                grad_policy_biases[i] += advantage * grad_p_b[i]\n",
    "        \n",
    "        # Apply gradient clipping and normalization\n",
    "        n_steps = len(states)\n",
    "        for i in range(len(grad_policy_weights)):\n",
    "            # Average gradients over episode\n",
    "            grad_policy_weights[i] /= n_steps\n",
    "            grad_policy_biases[i] /= n_steps\n",
    "            grad_value_weights[i] /= n_steps\n",
    "            grad_value_biases[i] /= n_steps\n",
    "            \n",
    "            # Clip gradients\n",
    "            grad_policy_weights[i] = jnp.clip(grad_policy_weights[i], -self.gradient_clip, self.gradient_clip)\n",
    "            grad_policy_biases[i] = jnp.clip(grad_policy_biases[i], -self.gradient_clip, self.gradient_clip)\n",
    "            grad_value_weights[i] = jnp.clip(grad_value_weights[i], -self.gradient_clip, self.gradient_clip)\n",
    "            grad_value_biases[i] = jnp.clip(grad_value_biases[i], -self.gradient_clip, self.gradient_clip)\n",
    "        \n",
    "        # Apply batch updates\n",
    "        new_policy_weights = [w + self.alpha_theta * gw for w, gw in zip(policy_weights, grad_policy_weights)]\n",
    "        new_policy_biases = [b + self.alpha_theta * gb for b, gb in zip(policy_biases, grad_policy_biases)]\n",
    "        new_value_weights = [w + self.alpha_w * gw for w, gw in zip(value_weights, grad_value_weights)]\n",
    "        new_value_biases = [b + self.alpha_w * gb for b, gb in zip(value_biases, grad_value_biases)]\n",
    "        \n",
    "        # Update parameters\n",
    "        self.policy_network.params = (new_policy_weights, new_policy_biases)\n",
    "        self.value_network.params = (new_value_weights, new_value_biases)\n",
    "        \n",
    "        # Compute training metrics (using original advantages for logging)\n",
    "        orig_advantages = jnp.array([float(returns[t] - baselines[t]) for t in range(len(states))])\n",
    "        avg_return = float(jnp.mean(returns))\n",
    "        avg_advantage = float(jnp.mean(orig_advantages))\n",
    "        avg_baseline = float(jnp.mean(jnp.array(baselines)))\n",
    "        advantage_std = float(jnp.std(orig_advantages) + 1e-8)\n",
    "        baseline_std = float(jnp.std(jnp.array(baselines)) + 1e-8)\n",
    "        \n",
    "        # Compute action statistics for logging\n",
    "        action_mean = float(jnp.mean(actions))\n",
    "        action_std = float(jnp.std(actions) + 1e-8)\n",
    "        action_min = float(jnp.min(actions))\n",
    "        action_max = float(jnp.max(actions))\n",
    "        \n",
    "        # Check for NaNs\n",
    "        if jnp.isnan(avg_advantage) or jnp.isnan(avg_baseline):\n",
    "            print(f\"WARNING: NaN detected!\")\n",
    "            return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        \n",
    "        return avg_return, avg_advantage, avg_baseline, advantage_std, baseline_std, action_mean, action_std, action_min, action_max\n",
    "\n",
    "# Test the updated REINFORCE with Gaussian policy for HalfCheetah\n",
    "agent = REINFORCE_Agent(\n",
    "    input_dim=17,                # HalfCheetah state space\n",
    "    policy_hidden_dims=[64, 64], \n",
    "    value_hidden_dims=[64, 32],\n",
    "    action_dim=6,                # HalfCheetah action space (6 joints)\n",
    "    learning_rate_theta=1e-4,    \n",
    "    learning_rate_w=5e-4,        \n",
    "    gamma=0.99,\n",
    "    gradient_clip=1.0            \n",
    ")\n",
    "\n",
    "# Test Gaussian action selection\n",
    "state = jnp.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7])  # 17-dim state\n",
    "print(\"Testing REINFORCE with Gaussian Policy for HalfCheetah:\")\n",
    "for i in range(3):\n",
    "    action = agent.select_action(state)\n",
    "    baseline_value = ValueNetwork.forward(state, agent.value_network.params)\n",
    "    mean, log_std = PolicyNetwork.forward(state, agent.policy_network.params, agent.action_dim)\n",
    "    print(f\"Action: {action}\")\n",
    "    print(f\"Policy mean: {mean}\")\n",
    "    print(f\"Policy log_std: {log_std}\")\n",
    "    print(f\"Estimated state value: {baseline_value:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Test update with dummy data\n",
    "dummy_states = jnp.array([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7],\n",
    "    [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8],\n",
    "    [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6]\n",
    "])\n",
    "dummy_actions = jnp.array([[0.1, -0.2, 0.3, -0.1, 0.0, 0.2], [0.0, 0.1, -0.3, 0.2, -0.1, 0.0], [-0.1, 0.0, 0.1, 0.0, 0.3, -0.2]])  # Continuous actions\n",
    "dummy_rewards = jnp.array([1.0, -0.5, 0.8])\n",
    "\n",
    "result = agent.update(dummy_states, dummy_actions, dummy_rewards)\n",
    "avg_return, avg_advantage, avg_baseline, advantage_std, baseline_std, action_mean, action_std, action_min, action_max = result\n",
    "\n",
    "print(f\"Average return: {avg_return:.4f}\")\n",
    "print(f\"Average advantage: {avg_advantage:.4f}\")\n",
    "print(f\"Average baseline: {avg_baseline:.4f}\")\n",
    "print(f\"Advantage std: {advantage_std:.4f}\")\n",
    "print(f\"Baseline std: {baseline_std:.4f}\")\n",
    "print(f\"Action mean: {action_mean:.4f}, std: {action_std:.4f}, min: {action_min:.4f}, max: {action_max:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting REINFORCE with Gaussian policy training...\n",
      "\n",
      "Environment: HalfCheetah-v5\n",
      "Observation space: 17\n",
      "Action space: 6 (continuous, Gaussian policy)\n",
      "Action range: -1.0 to 1.0\n",
      "Training for 50 episodes with 300 steps each\n",
      "Episode    1 | Reward:   -80.06 | Return:   -16.74 | Baseline:  -0.06 | Advantage:   -16.68 | Adv Std: 13.196 | Base Std:  0.116 | Act Mean:  0.004, Std:  0.616, Min: -2.288, Max:  2.222\n",
      "Episode    1 | Reward:   -80.06 | Return:   -16.74 | Baseline:  -0.06 | Advantage:   -16.68 | Adv Std: 13.196 | Base Std:  0.116 | Act Mean:  0.004, Std:  0.616, Min: -2.288, Max:  2.222\n",
      "Episode    2 | Reward:  -138.34 | Return:   -29.61 | Baseline:  -0.06 | Advantage:   -29.55 | Adv Std: 10.720 | Base Std:  0.109 | Act Mean: -0.019, Std:  0.611, Min: -2.111, Max:  2.033\n",
      "Episode    2 | Reward:  -138.34 | Return:   -29.61 | Baseline:  -0.06 | Advantage:   -29.55 | Adv Std: 10.720 | Base Std:  0.109 | Act Mean: -0.019, Std:  0.611, Min: -2.111, Max:  2.033\n",
      "Episode    3 | Reward:    -9.29 | Return:     3.49 | Baseline:  -0.06 | Advantage:     3.55 | Adv Std:  8.221 | Base Std:  0.117 | Act Mean: -0.000, Std:  0.598, Min: -1.880, Max:  1.983\n",
      "Episode    3 | Reward:    -9.29 | Return:     3.49 | Baseline:  -0.06 | Advantage:     3.55 | Adv Std:  8.221 | Base Std:  0.117 | Act Mean: -0.000, Std:  0.598, Min: -1.880, Max:  1.983\n",
      "Episode    4 | Reward:   -44.53 | Return:   -15.01 | Baseline:  -0.05 | Advantage:   -14.96 | Adv Std: 11.620 | Base Std:  0.124 | Act Mean:  0.017, Std:  0.608, Min: -2.146, Max:  2.068\n",
      "Episode    4 | Reward:   -44.53 | Return:   -15.01 | Baseline:  -0.05 | Advantage:   -14.96 | Adv Std: 11.620 | Base Std:  0.124 | Act Mean:  0.017, Std:  0.608, Min: -2.146, Max:  2.068\n",
      "Episode    5 | Reward:   -43.33 | Return:   -14.79 | Baseline:  -0.05 | Advantage:   -14.74 | Adv Std: 11.125 | Base Std:  0.108 | Act Mean: -0.018, Std:  0.609, Min: -2.173, Max:  1.877\n",
      "Episode    5 | Reward:   -43.33 | Return:   -14.79 | Baseline:  -0.05 | Advantage:   -14.74 | Adv Std: 11.125 | Base Std:  0.108 | Act Mean: -0.018, Std:  0.609, Min: -2.173, Max:  1.877\n",
      "Episode    6 | Reward:   -85.92 | Return:   -17.92 | Baseline:  -0.05 | Advantage:   -17.86 | Adv Std:  6.899 | Base Std:  0.108 | Act Mean:  0.013, Std:  0.606, Min: -1.944, Max:  2.100\n",
      "Episode    6 | Reward:   -85.92 | Return:   -17.92 | Baseline:  -0.05 | Advantage:   -17.86 | Adv Std:  6.899 | Base Std:  0.108 | Act Mean:  0.013, Std:  0.606, Min: -1.944, Max:  2.100\n",
      "Episode    7 | Reward:  -123.11 | Return:   -27.10 | Baseline:  -0.05 | Advantage:   -27.06 | Adv Std: 11.211 | Base Std:  0.113 | Act Mean: -0.003, Std:  0.614, Min: -2.239, Max:  1.879\n",
      "Episode    7 | Reward:  -123.11 | Return:   -27.10 | Baseline:  -0.05 | Advantage:   -27.06 | Adv Std: 11.211 | Base Std:  0.113 | Act Mean: -0.003, Std:  0.614, Min: -2.239, Max:  1.879\n",
      "Episode    8 | Reward:  -144.12 | Return:   -37.27 | Baseline:  -0.04 | Advantage:   -37.22 | Adv Std: 13.947 | Base Std:  0.107 | Act Mean:  0.010, Std:  0.596, Min: -2.163, Max:  2.303\n",
      "Episode    8 | Reward:  -144.12 | Return:   -37.27 | Baseline:  -0.04 | Advantage:   -37.22 | Adv Std: 13.947 | Base Std:  0.107 | Act Mean:  0.010, Std:  0.596, Min: -2.163, Max:  2.303\n",
      "Episode    9 | Reward:  -129.64 | Return:   -30.75 | Baseline:  -0.03 | Advantage:   -30.72 | Adv Std: 14.005 | Base Std:  0.109 | Act Mean:  0.007, Std:  0.616, Min: -2.032, Max:  2.190\n",
      "Episode    9 | Reward:  -129.64 | Return:   -30.75 | Baseline:  -0.03 | Advantage:   -30.72 | Adv Std: 14.005 | Base Std:  0.109 | Act Mean:  0.007, Std:  0.616, Min: -2.032, Max:  2.190\n",
      "\n",
      "Training interrupted by user.\n",
      "\n",
      "Training interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Quick test configuration for HalfCheetah with Gaussian policy\n",
    "CONFIG = {\n",
    "    'gamma': 0.99,\n",
    "    'episode_length': 300,       # Increased episode length for better learning\n",
    "    'episodes': 50,              # Quick test with 50 episodes\n",
    "    'learning_rate_theta': 1e-4, # Reduced policy learning rate\n",
    "    'learning_rate_w': 2e-4,     # Reduced value function learning rate\n",
    "    'policy_hidden_dims': [16, 16],    # Larger policy network\n",
    "    'value_hidden_dims': [16, 16],     # Larger value network\n",
    "    'log_interval': 1,          # Log every 10 episodes\n",
    "    'gradient_clip': 0.5         # Reduced gradient clipping\n",
    "}\n",
    "\n",
    "def run_demo_episode(agent, env_render, episode_num):\n",
    "    \"\"\"Run a single demonstration episode with rendering.\"\"\"\n",
    "    print(f\"\\n--- Demonstration Episode {episode_num} ---\")\n",
    "    observation, _ = env_render.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    episode_actions = []\n",
    "    \n",
    "    for step in range(CONFIG['episode_length']):\n",
    "        # Convert observation to JAX array\n",
    "        state = jnp.array(observation, dtype=jnp.float32)\n",
    "        \n",
    "        # Select action using Gaussian policy\n",
    "        action = agent.select_action(state)\n",
    "        episode_actions.append(action)\n",
    "        \n",
    "        # Take step in environment\n",
    "        observation, reward, terminated, truncated, _ = env_render.step(np.array(action))\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Action statistics for demo episode\n",
    "    episode_actions = jnp.stack(episode_actions)\n",
    "    action_mean = float(jnp.mean(episode_actions))\n",
    "    action_std = float(jnp.std(episode_actions))\n",
    "    \n",
    "    print(f\"Demo episode reward: {total_reward:.2f} (steps: {step_count})\")\n",
    "    print(f\"Demo action mean: {action_mean:.3f}, std: {action_std:.3f}\")\n",
    "    return total_reward\n",
    "\n",
    "def train_agent():\n",
    "    \"\"\"Train REINFORCE agent with Gaussian policy on HalfCheetah.\"\"\"\n",
    "    # Training environment\n",
    "    env = gym.make(\"HalfCheetah-v5\", render_mode='human')\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    print(f\"Environment: HalfCheetah-v5\")\n",
    "    print(f\"Observation space: {obs_dim}\")\n",
    "    print(f\"Action space: {action_dim} (continuous, Gaussian policy)\")\n",
    "    print(f\"Action range: {env.action_space.low[0]:.1f} to {env.action_space.high[0]:.1f}\")\n",
    "    print(f\"Training for {CONFIG['episodes']} episodes with {CONFIG['episode_length']} steps each\")\n",
    "    \n",
    "    # Create agent with Gaussian policy\n",
    "    agent = REINFORCE_Agent(\n",
    "        input_dim=obs_dim,\n",
    "        policy_hidden_dims=CONFIG['policy_hidden_dims'],\n",
    "        value_hidden_dims=CONFIG['value_hidden_dims'],\n",
    "        action_dim=action_dim,\n",
    "        learning_rate_theta=CONFIG['learning_rate_theta'],\n",
    "        learning_rate_w=CONFIG['learning_rate_w'],\n",
    "        gamma=CONFIG['gamma'],\n",
    "        gradient_clip=CONFIG['gradient_clip']\n",
    "    )\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(CONFIG['episodes']):\n",
    "        observation, _ = env.reset()\n",
    "        \n",
    "        # Collect trajectory\n",
    "        states, actions_list, rewards = [], [], []\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(CONFIG['episode_length']):\n",
    "            # Convert observation to JAX array\n",
    "            state = jnp.array(observation, dtype=jnp.float32)\n",
    "            \n",
    "            # Select action using Gaussian policy\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Store transition data\n",
    "            states.append(state)\n",
    "            actions_list.append(action)\n",
    "            \n",
    "            # Take step in environment\n",
    "            observation, reward, terminated, truncated, _ = env.step(np.array(action))\n",
    "            rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Convert to JAX arrays\n",
    "        states = jnp.stack(states)\n",
    "        actions = jnp.stack(actions_list)\n",
    "        rewards = jnp.array(rewards)\n",
    "        \n",
    "        # Update policy using REINFORCE with baseline\n",
    "        result = agent.update(states, actions, rewards)\n",
    "        avg_return, avg_advantage, avg_baseline, advantage_std, baseline_std, action_mean, action_std, action_min, action_max = result\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Log progress with action statistics (FIXED: single print statement)\n",
    "        if (episode + 1) % CONFIG['log_interval'] == 0:\n",
    "            recent_reward = np.mean(episode_rewards[-CONFIG['log_interval']])\n",
    "            \n",
    "            print(f\"Episode {episode + 1:4d} | Reward: {recent_reward:8.2f} | Return: {avg_return:8.2f} | Baseline: {avg_baseline:6.2f} | Advantage: {avg_advantage:8.2f} | Adv Std: {advantage_std:6.3f} | Base Std: {baseline_std:6.3f} | Act Mean: {action_mean:6.3f}, Std: {action_std:6.3f}, Min: {action_min:6.3f}, Max: {action_max:6.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return episode_rewards\n",
    "\n",
    "# Start training with Gaussian policy\n",
    "try:\n",
    "    print(\"Starting REINFORCE with Gaussian policy training...\\n\")\n",
    "    \n",
    "    training_rewards = train_agent()\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Final average training reward: {np.mean(training_rewards[-10:]):.2f}\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(\"This might be due to environment setup issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
